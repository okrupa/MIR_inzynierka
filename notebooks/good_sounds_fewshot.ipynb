{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1qn5aXRCtfD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch\n",
        "!pip install pytorch-lightning\n",
        "!pip install numpy\n",
        "!pip install --no-cache-dir --upgrade music-fsl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Variables\n"
      ],
      "metadata": {
        "id": "oyRTt8y8adsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rate = 16000 # sample rate of the audio\n",
        "n_way= 5 # number of classes per episode\n",
        "n_support = 5 # number of support examples per class\n",
        "n_query = 20 # number of samples per class to use as query\n",
        "n_train_episodes = int(1000) # number of episodes to generate for training\n",
        "n_val_episodes = 50 # number of episodes to generate for validation\n",
        "num_workers = 10 # number of workers to use for data loading"
      ],
      "metadata": {
        "id": "6vmFJAJVDIDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Good_sounds"
      ],
      "metadata": {
        "id": "stYp2FICag5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "\n",
        "from music_fsl.backbone import Backbone\n",
        "from music_fsl.protonet import PrototypicalNet"
      ],
      "metadata": {
        "id": "l3FKt9ZrC93g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class ClassConditionalDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict[Any, Any]:\n",
        "        \"\"\"\n",
        "        Grab an item from the dataset. The item returned must be a dictionary. \n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    @property\n",
        "    def classlist(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        The classlist property returns a list of class labels available in the dataset.\n",
        "        This property enables users of the dataset to easily access a list of all the classes in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: A list of class labels available in the dataset. \n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def class_to_indices(self) -> Dict[str, List[int]]:\n",
        "        \"\"\"\n",
        "        Returns a dictionary where the keys are class labels and the values are \n",
        "        lists of indices in the dataset that belong to that class. \n",
        "        This property enables users of the dataset to easily access \n",
        "        examples that belong to specific classes. \n",
        "\n",
        "        Implement me!\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, List[int]]: A dictionary mapping class labels to lists of dataset indices. \n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "tdjHjTI1pm8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import defaultdict\n",
        "# import mirdata\n",
        "# import librosa\n",
        "# import music_fsl.util as util\n",
        "# from typing import List, Dict\n",
        "\n",
        "\n",
        "# class GoodSounds(ClassConditionalDataset):\n",
        "#     \"\"\"\n",
        "#     Initialize a `GoodSounds Dataset Loader` dataset instance.\n",
        "    \n",
        "#     Args:\n",
        "#         instruments (List[str]): A list of instruments to include in the dataset.\n",
        "#         duration (float): The duration of each audio clip in the dataset (in seconds).\n",
        "#         sample_rate (int): The sample rate of the audio clips in the dataset (in Hz).\n",
        "#         dataset - loaded mirdata.dataset\n",
        "#     \"\"\"\n",
        "\n",
        "#     INSTRUMENTS = [\n",
        "#         'flute', 'cello', 'clarinet', 'trumpet', 'violin', 'sax_alto', 'sax_tenor', 'sax_baritone', 'sax_soprano', 'oboe', 'piccolo', 'bass'\n",
        "#     ]\n",
        "\n",
        "#     def __init__(self, \n",
        "#             instruments: List[str] = None,\n",
        "#             duration: float = 1.0, \n",
        "#             sample_rate: int = 16000,\n",
        "#             dataset = None,\n",
        "#         ):\n",
        "#         if instruments is None:\n",
        "#             instruments = self.INSTRUMENTS\n",
        "\n",
        "#         self.instruments = instruments  \n",
        "#         self.duration = duration\n",
        "#         self.sample_rate = sample_rate\n",
        "\n",
        "#         # initialize the medley_solos_db dataset and download if necessary\n",
        "#         if dataset is not None:\n",
        "#             self.dataset = dataset\n",
        "#         else:\n",
        "#           self.dataset = mirdata.initialize('medley_solos_db')\n",
        "#           self.dataset.download()\n",
        "\n",
        "#         # make sure the instruments passed in are valid\n",
        "#         for instrument in instruments:\n",
        "#             assert instrument in self.INSTRUMENTS, f\"{instrument} is not a valid instrument\"\n",
        "\n",
        "#         # load all tracks for this instrument\n",
        "#         self.tracks = []\n",
        "#         i = 0\n",
        "#         for track in self.dataset.load_tracks().values():\n",
        "#             if track.instrument in self.instruments:\n",
        "#               if librosa.get_duration(filename=track.audio_path) >= duration:\n",
        "#                 self.tracks.append(track)\n",
        "\n",
        "\n",
        "#     @property\n",
        "#     def classlist(self) -> List[str]:\n",
        "#         return self.instruments\n",
        "\n",
        "#     @property\n",
        "#     def class_to_indices(self) -> Dict[str, List[int]]:\n",
        "#         # cache it in self._class_to_indices \n",
        "#         # so we don't have to recompute it every time\n",
        "#         if not hasattr(self, \"_class_to_indices\"):\n",
        "#             self._class_to_indices = defaultdict(list)\n",
        "#             for i, track in enumerate(self.tracks):\n",
        "#                 self._class_to_indices[track.instrument].append(i)\n",
        "\n",
        "#         return self._class_to_indices\n",
        "\n",
        "#     def __getitem__(self, index) -> Dict:\n",
        "#         # load the track for this index\n",
        "#         track = self.tracks[index]\n",
        "\n",
        "#         # load the excerpt\n",
        "#         data = util.load_excerpt(track.audio_path, self.duration, self.sample_rate)\n",
        "#         data[\"label\"] = track.instrument\n",
        "\n",
        "#         return data\n",
        "\n",
        "#     def __len__(self) -> int:\n",
        "#         return len(self.tracks)"
      ],
      "metadata": {
        "id": "Gs5g37k5pjou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GoodSounds(ClassConditionalDataset):\n",
        "    \"\"\"\n",
        "    Initialize a `GoodSounds Dataset Loader` dataset instance.\n",
        "    \n",
        "    Args:\n",
        "        instruments (List[str]): A list of instruments to include in the dataset.\n",
        "        duration (float): The duration of each audio clip in the dataset (in seconds).\n",
        "        sample_rate (int): The sample rate of the audio clips in the dataset (in Hz).\n",
        "        dataset - loaded mirdata.dataset\n",
        "    \"\"\"\n",
        "\n",
        "    INSTRUMENTS = [\n",
        "        'flute', 'cello', 'clarinet', 'trumpet', 'violin', 'sax_alto', 'sax_tenor', 'sax_baritone', 'sax_soprano', 'oboe', 'piccolo', 'bass'\n",
        "    ]\n",
        "\n",
        "    def __init__(self, \n",
        "            instruments: List[str] = None,\n",
        "            duration: float = 1.0, \n",
        "            sample_rate: int = 16000,\n",
        "            dataset_path: str = None\n",
        "        ):\n",
        "        if instruments is None:\n",
        "            instruments = self.INSTRUMENTS\n",
        "\n",
        "        self.instruments = instruments  \n",
        "        self.duration = duration\n",
        "        self.sample_rate = sample_rate\n",
        "        self.dataset_path = dataset_path\n",
        "\n",
        "        # make sure the instruments passed in are valid\n",
        "        for instrument in instruments:\n",
        "            assert instrument in self.INSTRUMENTS, f\"{instrument} is not a valid instrument\"\n",
        "\n",
        "        # load all tracks for this instrument\n",
        "        self.tracks = []\n",
        "        for dir in os.listdir(self.dataset_path):\n",
        "            ins = dir.split('_')[0]\n",
        "            if ins in self.instruments:\n",
        "                for subdir_dir, dirs_dir, files_dir in os.walk(os.path.join(self.dataset_path, dir, 'neumann')):\n",
        "                    for file in files_dir:\n",
        "                        if file.endswith('.wav'):\n",
        "                            if librosa.get_duration(filename=os.path.join(self.dataset_path, dir, 'neumann', file)) >= duration:\n",
        "                                self.tracks.append([os.path.join(self.dataset_path, dir, 'neumann', file), ins])\n",
        "            else:\n",
        "                ins = f'{ins}_{dir.split(\"_\")[1]}'\n",
        "                if ins in self.instruments:\n",
        "                    for subdir_dir, dirs_dir, files_dir in os.walk(os.path.join(self.dataset_path, dir, 'neumann')):\n",
        "                        for file in files_dir:\n",
        "                            if file.endswith('.wav'):\n",
        "                                if librosa.get_duration(filename=os.path.join(self.dataset_path, dir, 'neumann', file)) >= duration:\n",
        "                                    self.tracks.append([os.path.join(self.dataset_path, dir, 'neumann', file), ins])\n",
        "\n",
        "\n",
        "    @property\n",
        "    def classlist(self) -> List[str]:\n",
        "        return self.instruments\n",
        "\n",
        "    @property\n",
        "    def class_to_indices(self) -> Dict[str, List[int]]:\n",
        "        # cache it in self._class_to_indices \n",
        "        # so we don't have to recompute it every time\n",
        "        if not hasattr(self, \"_class_to_indices\"):\n",
        "            self._class_to_indices = defaultdict(list)\n",
        "            for i, track in enumerate(self.tracks):\n",
        "                self._class_to_indices[track[1]].append(i)\n",
        "\n",
        "        return self._class_to_indices\n",
        "\n",
        "    def __getitem__(self, index) -> Dict:\n",
        "        # load the track for this index\n",
        "        track = self.tracks[index]\n",
        "\n",
        "        # load the excerpt\n",
        "        data = util.load_excerpt(track[0], self.duration, self.sample_rate)\n",
        "        data[\"label\"] = track[1]\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.tracks)"
      ],
      "metadata": {
        "id": "PHHlf7qpnHoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from music_fsl.data import ClassConditionalDataset\n",
        "import music_fsl.util as util\n",
        "\n",
        "from typing import Tuple, Dict\n",
        "class EpisodeDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "        A dataset for sampling few-shot learning tasks from a class-conditional dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset (ClassConditionalDataset): The dataset to sample episodes from.\n",
        "        n_way (int): The number of classes to sample per episode.\n",
        "            Default: 5.\n",
        "        n_support (int): The number of samples per class to use as support.\n",
        "            Default: 5.\n",
        "        n_query (int): The number of samples per class to use as query.\n",
        "            Default: 20.\n",
        "        n_episodes (int): The number of episodes to generate.\n",
        "            Default: 100.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "        dataset: ClassConditionalDataset, \n",
        "        n_way: int = 5, \n",
        "        n_support: int = 5,\n",
        "        n_query: int = 20,\n",
        "        n_episodes: int = 100,\n",
        "    ):\n",
        "        self.dataset = dataset\n",
        "\n",
        "        self.n_way = n_way\n",
        "        self.n_support = n_support\n",
        "        self.n_query = n_query\n",
        "        self.n_episodes = n_episodes\n",
        "    \n",
        "    def __getitem__(self, index: int) -> Tuple[Dict, Dict]:\n",
        "        \"\"\"Sample an episode from the class-conditional dataset. \n",
        "\n",
        "        Each episode is a tuple of two dictionaries: a support set and a query set.\n",
        "        The support set contains a set of samples from each of the classes in the\n",
        "        episode, and the query set contains another set of samples from each of the\n",
        "        classes. The class labels are added to each item in the support and query\n",
        "        sets, and the list of classes is also included in each dictionary.\n",
        "\n",
        "        Yields:\n",
        "            Tuple[Dict[str, Any], Dict[str, Any]]: A tuple containing the support\n",
        "            set and the query set for an episode.\n",
        "        \"\"\"\n",
        "        # seed the random number generator so we can reproduce this episode\n",
        "        rng = random.Random(index)\n",
        "\n",
        "        # sample the list of classes for this episode\n",
        "        episode_classlist = rng.sample(self.dataset.classlist, self.n_way)\n",
        "\n",
        "        # sample the support and query sets for this episode\n",
        "        support, query = [], []\n",
        "        for c in episode_classlist:\n",
        "            # grab the dataset indices for this class\n",
        "            all_indices = self.dataset.class_to_indices[c]\n",
        "            # sample the support and query sets for this class\n",
        "            indices = rng.sample(all_indices, self.n_support + self.n_query)\n",
        "            items = [self.dataset[i] for i in indices]\n",
        "\n",
        "            # add the class label to each item\n",
        "            for item in items:\n",
        "                item[\"target\"] = torch.tensor(episode_classlist.index(c))\n",
        "\n",
        "            # split the support and query sets\n",
        "            support.extend(items[:self.n_support])\n",
        "            query.extend(items[self.n_support:])\n",
        "\n",
        "        # collate the support and query sets\n",
        "        support = util.collate_list_of_dicts(support)\n",
        "        query = util.collate_list_of_dicts(query)\n",
        "\n",
        "        support[\"classlist\"] = episode_classlist\n",
        "        query[\"classlist\"] = episode_classlist\n",
        "        \n",
        "        return support, query\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_episodes\n",
        "\n",
        "    def print_episode(self, support, query):\n",
        "        \"\"\"Print a summary of the support and query sets for an episode.\n",
        "\n",
        "        Args:\n",
        "            support (Dict[str, Any]): The support set for an episode.\n",
        "            query (Dict[str, Any]): The query set for an episode.\n",
        "        \"\"\"\n",
        "        print(\"Support Set:\")\n",
        "        print(f\"  Classlist: {support['classlist']}\")\n",
        "        print(f\"  Audio Shape: {support['audio'].shape}\")\n",
        "        print(f\"  Target Shape: {support['target'].shape}\")\n",
        "        print()\n",
        "        print(\"Query Set:\")\n",
        "        print(f\"  Classlist: {query['classlist']}\")\n",
        "        print(f\"  Audio Shape: {query['audio'].shape}\")\n",
        "        print(f\"  Target Shape: {query['target'].shape}\")\n"
      ],
      "metadata": {
        "id": "ofSaPTzuh3UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LwuEs4MlgEND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TRAIN_INSTRUMENTS = [\n",
        "       'cello', 'clarinet', 'violin', 'sax_alto', 'sax_baritone', 'sax_soprano', 'piccolo',\n",
        "    ]\n",
        "\n",
        "TEST_INSTRUMENTS = [\n",
        "        'flute', 'trumpet', 'sax_tenor', 'oboe', 'bass'\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "FSc3wagoD32S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the datasets\n",
        "train_data = GoodSounds(\n",
        "    instruments=TRAIN_INSTRUMENTS, \n",
        "    sample_rate=sample_rate,\n",
        "    dataset_path = '/content/drive/MyDrive/good_sounds/sound_files'\n",
        ")\n",
        "\n",
        "val_data = GoodSounds(\n",
        "    instruments=TEST_INSTRUMENTS, \n",
        "    sample_rate=sample_rate,\n",
        "    dataset_path = '/content/drive/MyDrive/good_sounds/sound_files'\n",
        ")"
      ],
      "metadata": {
        "id": "0-pbaLNiEcPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The dataset has {len(train_data)} examples.\")\n",
        "print(f\"The dataset has {len(train_data.classlist)} classes.\\n\")\n",
        "\n",
        "# print the number of examples for each class\n",
        "for instrument, indices in train_data.class_to_indices.items():\n",
        "    print(f\"{instrument} has {len(indices)} examples\")\n",
        "\n",
        "print(f\"The dataset has {len(val_data)} examples.\")\n",
        "print(f\"The dataset has {len(val_data.classlist)} classes.\\n\")\n",
        "\n",
        "# print the number of examples for each class\n",
        "for instrument, indices in val_data.class_to_indices.items():\n",
        "    print(f\"{instrument} has {len(indices)} examples\")"
      ],
      "metadata": {
        "id": "scyIFxHkOUn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the episode datasets\n",
        "train_episodes = EpisodeDataset(\n",
        "    dataset=train_data, \n",
        "    n_way=n_way, \n",
        "    n_support=n_support,\n",
        "    n_query=n_query, \n",
        "    n_episodes=n_train_episodes\n",
        ")\n",
        "\n",
        "val_episodes = EpisodeDataset(\n",
        "    dataset=val_data, \n",
        "    n_way=n_way, \n",
        "    n_support=n_support,\n",
        "    n_query=n_query, \n",
        "    n_episodes=n_val_episodes\n",
        ")"
      ],
      "metadata": {
        "id": "nyArgn1GItOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(\n",
        "    train_episodes, \n",
        "    batch_size=None,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_episodes, \n",
        "    batch_size=None,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "OCQ_lZYiIxvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build models\n",
        "backbone = Backbone(sample_rate=sample_rate)\n",
        "protonet = PrototypicalNet(backbone)\n"
      ],
      "metadata": {
        "id": "GFRabyVEI34M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FewShotLearner(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, \n",
        "        protonet: nn.Module, \n",
        "        learning_rate: float = 0.001,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.protonet = protonet\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        self.metrics = nn.ModuleDict({\n",
        "            'accuracy': Accuracy()\n",
        "        })\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def step(self, batch, batch_idx, tag: str):\n",
        "        support, query = batch\n",
        "\n",
        "        logits = self.protonet(support, query)\n",
        "        loss = self.loss(logits, query[\"target\"])\n",
        "\n",
        "        output = {\"loss\": loss}\n",
        "        for k, metric in self.metrics.items():\n",
        "            output[k] = metric(logits, query[\"target\"])\n",
        "\n",
        "        for k, v in output.items():\n",
        "            self.log(f\"{k}/{tag}\", v)\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.step(batch, batch_idx, \"train\")\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.step(batch, batch_idx, \"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.step(batch, batch_idx, \"test\")"
      ],
      "metadata": {
        "id": "oVCKcRVBI_cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learner = FewShotLearner(protonet)"
      ],
      "metadata": {
        "id": "yctwKz5DJCZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train\n"
      ],
      "metadata": {
        "id": "WHca4NdFhQ7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.profiler import SimpleProfiler\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    gpus=1 if torch.cuda.is_available() else 0,\n",
        "    max_epochs=1,\n",
        "    log_every_n_steps=1, \n",
        "    val_check_interval=50,\n",
        "    profiler=SimpleProfiler(\n",
        "        filename=\"profile.txt\",\n",
        "    ), \n",
        "    logger=TensorBoardLogger(\n",
        "        save_dir=\".\",\n",
        "        name=\"logs\"\n",
        "    ), \n",
        ")\n",
        "\n",
        "# train!\n",
        "trainer.fit(learner, train_loader, val_dataloaders=val_loader)"
      ],
      "metadata": {
        "id": "19fZxmIvJGxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save"
      ],
      "metadata": {
        "id": "lP6unnw_hbm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/good_sounds_5_5_001.zip /content/logs"
      ],
      "metadata": {
        "id": "KRXX8AXWclIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/good_sounds_5_5_001.zip') "
      ],
      "metadata": {
        "id": "Btoi0wnWnapB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tensorboard"
      ],
      "metadata": {
        "id": "yWKZWWeYhmE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard"
      ],
      "metadata": {
        "id": "2Ibi8DBbcv49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "k4mk6MpWdbUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard --logdir /content/logs"
      ],
      "metadata": {
        "id": "wS1jmvc3Yqc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data visualisation"
      ],
      "metadata": {
        "id": "Y-EDLctGhyIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"torchmetrics==0.10.2\" \n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "eHA3i1bwzpN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "from music_fsl.util import dim_reduce, embedding_plot, batch_device"
      ],
      "metadata": {
        "id": "tqoaL5_MoIzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/logs/version_0/checkpoints/epoch=0-step=1000.ckpt\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "sample_rate = 16000"
      ],
      "metadata": {
        "id": "--U183PwoQ1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "protonet = PrototypicalNet(Backbone(sample_rate))\n",
        "learner = FewShotLearner.load_from_checkpoint(checkpoint_path, protonet=protonet)\n",
        "learner.eval()\n",
        "learner = learner.to(DEVICE)"
      ],
      "metadata": {
        "id": "sW4fE_3foV7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_query = 15\n",
        "n_episodes = 50 \n",
        "\n",
        "dataset = GoodSounds(\n",
        "    instruments=TEST_INSTRUMENTS, \n",
        "    sample_rate=sample_rate,\n",
        "    dataset_path = '/content/drive/MyDrive/good_sounds/sound_files'\n",
        ")\n",
        "# load our evaluation data\n",
        "test_episodes = EpisodeDataset(\n",
        "    dataset=dataset, \n",
        "    n_way=n_way, \n",
        "    n_support=n_support,\n",
        "    n_query=n_query, \n",
        "    n_episodes=n_episodes\n",
        ")"
      ],
      "metadata": {
        "id": "l539ifqmoYEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = Accuracy(num_classes=n_way, average=\"samples\")"
      ],
      "metadata": {
        "id": "i9rmF4QRolii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collect all the embeddings in the test set\n",
        "# so we can plot them later\n",
        "embedding_table = []\n",
        "pbar = tqdm.tqdm(range(len(test_episodes)))\n",
        "for episode_idx in pbar:\n",
        "    support, query = test_episodes[episode_idx]\n",
        "\n",
        "    # move all tensors to cuda if necessary\n",
        "    batch_device(support, DEVICE)\n",
        "    batch_device(query, DEVICE)\n",
        "\n",
        "    # get the embeddings\n",
        "    logits = learner.protonet(support, query)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc = metric(logits, query[\"target\"])\n",
        "    pbar.set_description(f\"Episode {episode_idx} // Accuracy: {acc.item():.2f}\")\n",
        "\n",
        "    # add all the support and query embeddings to our records\n",
        "    for subset_idx, subset in enumerate((support, query)):\n",
        "        for emb, label in zip(subset[\"embeddings\"], subset[\"target\"]):\n",
        "            embedding_table.append({\n",
        "                \"embedding\": emb.detach().cpu().numpy(),\n",
        "                \"label\": support[\"classlist\"][label],\n",
        "                \"marker\": (\"support\", \"query\")[subset_idx], \n",
        "                \"episode_idx\": episode_idx\n",
        "            })\n",
        "        \n",
        "    # also add the prototype embeddings to our records\n",
        "    for class_idx, emb in enumerate(support[\"prototypes\"]):\n",
        "        embedding_table.append({\n",
        "            \"embedding\": emb.detach().cpu().numpy(),\n",
        "            \"label\": support[\"classlist\"][class_idx],\n",
        "            \"marker\": \"prototype\", \n",
        "            \"episode_idx\": episode_idx\n",
        "        })"
      ],
      "metadata": {
        "id": "nTgq5jj5olt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the total accuracy across all episodes\n",
        "total_acc = metric.compute()\n",
        "print(f\"Total accuracy, averaged across all episodes: {total_acc:.2f}\")"
      ],
      "metadata": {
        "id": "cfeG86w_oqbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform a TSNE over all embeddings in the test dataset\n",
        "embeddings = dim_reduce(\n",
        "    embeddings=np.stack([d[\"embedding\"] for d in embedding_table]),\n",
        "    method=\"tsne\",\n",
        "    n_components=2,\n",
        ")\n",
        "\n",
        "# replace the original 512-dim embeddings with the 2-dim tsne embeddings\n",
        "# in our embedding table\n",
        "for entry, dim_reduced_embedding in zip(embedding_table, embeddings):\n",
        "    entry[\"embedding\"] = dim_reduced_embedding"
      ],
      "metadata": {
        "id": "Clf1pexuotpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = embedding_plot(\n",
        "    proj=np.stack([d[\"embedding\"] for d in embedding_table]),\n",
        "    color_labels=[d[\"label\"] for d in embedding_table],\n",
        "    marker_labels=[d[\"marker\"] for d in embedding_table],\n",
        "    title=\"IRMAS Protonet Embeddings\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "FiSPhZP9ovXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episode_idx = 5\n",
        "\n",
        "subtable = [d for d in embedding_table if d[\"episode_idx\"] == episode_idx]\n",
        "\n",
        "fig = embedding_plot(\n",
        "    proj=np.stack([d[\"embedding\"] for d in subtable]),\n",
        "    color_labels=[d[\"label\"] for d in subtable],\n",
        "    marker_labels=[d[\"marker\"] for d in subtable],\n",
        "    title=f\"episode {episode_idx} -- embeddings\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "FGSNQr3XoxmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}